---
title: "DAB501 Final Project"
author: "Group 02 | Section 004 | Joel Addala, Chris Chhotai, Shreyansh Patel"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---

## Student Information

- Name: Joel Addala
- ID: 823570
- Name: Chris Chhotai
- ID: 826416
- Name: Shreyansh Patel
- ID: 790426

### Acknowledgement 

We, Joel Addala, Chris Chhotai, and Shreyansh Patel, hereby state that we have not communicated with or gained information in any way from any person or resource that would violate the College’s academic integrity policies, and that all work presented is our own. In addition, we also agree not to share our work in any way, before or after submission, that would violate the College’s academic integrity policies

## Packages and Data

### R and R Studio Version

R : R version 4.2.2 (2022-10-31 ucrt)

RStudio 2023.03.0+386 "Cherry Blossom" Release (3c53477afb13ab959aeb5b34df1f10c237b256c3, 2023-03-09) for Windows
Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) RStudio/2023.03.0+386 Chrome/108.0.5359.179 Electron/22.0.3 Safari/537.36

### R Packages Used

* ggplot2 (3.4.0)
* ggthemes (4.2.4)
* naniar (1.0.0)
* dplyr (1.0.10)
* caret (6.0.93)
* caTools (1.18.2)
* MASS (7.3.58.1)
* rsq (2.5)
* car (3.1.2)
* pROC (1.18.0)

```{r Load Packages, message=FALSE, warning=FALSE}
library(ggplot2)
library(ggthemes)
library(naniar)
library(dplyr)
library(caret)
library(caTools)
library(MASS)
library(rsq)
library(car)
library(pROC)  
```

### Load Data

```{r}
data = read.csv("Heart Disease Health Indicator.csv")
```

### Cleaning

#### Null Values

```{r}
n_miss(data)
```
```{r}
any_na(data)
```
```{r}
miss_var_summary(data)
```

There is no Missing values or NAs, so we do not need to deal with missing data.

#### Balance Data

```{r}
prop.table(table(data$HeartDiseaseorAttack))
```

The Data is imbalanced, therefore we use _downsample()_ method from _caret_ package to balance the majority data equivalent to minority data.

```{r}
clean_data <- downSample(x = data[, -ncol(data)], y=factor(data$HeartDiseaseorAttack))
```

```{r}
prop.table(table(clean_data$HeartDiseaseorAttack))
```

#### Data Types

```{r}
str(clean_data)
```

Converting all factors from numeric data types to factor data type:

```{r}
clean_data$HeartDiseaseorAttack <- as.factor(clean_data$HeartDiseaseorAttack)
clean_data$HighBP <- as.factor(clean_data$HighBP)
clean_data$HighChol <- as.factor(clean_data$HighChol)
clean_data$CholCheck <- as.factor(clean_data$CholCheck)
clean_data$Smoker <- as.factor(clean_data$Smoker)
clean_data$Stroke <- as.factor(clean_data$Stroke)
clean_data$Diabetes <- as.factor(clean_data$Diabetes)
clean_data$PhysActivity <- as.factor(clean_data$PhysActivity)
clean_data$Fruits <- as.factor(clean_data$Fruits)
clean_data$Veggies <- as.factor(clean_data$Veggies)
clean_data$HvyAlcoholConsump <- as.factor(clean_data$HvyAlcoholConsump)
clean_data$AnyHealthcare <- as.factor(clean_data$AnyHealthcare)
clean_data$NoDocbcCost <- as.factor(clean_data$NoDocbcCost)
clean_data$GenHlth <- as.factor(clean_data$GenHlth)
clean_data$DiffWalk <- as.factor(clean_data$DiffWalk)
clean_data$Sex <- as.factor(clean_data$Sex)
clean_data$Age <- as.factor(clean_data$Age)
clean_data$Education <- as.factor(clean_data$Education)
clean_data$Class <- as.factor(clean_data$Class)
```

```{r}
str(clean_data)
```

#### Outliers in BMI

```{r}
summary(clean_data$BMI)
```

Remove Abnormal BMI.

```{r}
BMI_q1 = quantile(clean_data$BMI, 0.25)
BMI_q2 = quantile(clean_data$BMI, 0.75)

BMI_iqr = IQR(clean_data$BMI)

bmi_lower_limit = BMI_q1 - (1.5 * BMI_iqr)
bmi_upper_limit = BMI_q2 + (1.5 * BMI_iqr)

clean_data <- clean_data %>% filter(BMI >= bmi_lower_limit & BMI <= bmi_upper_limit)
```

```{r}
summary(clean_data$BMI)
```

Moving forward we will be using this clean and balanced data set for modeling.

## MODELING: Probability of Heart Disease or Attack using Logistic Regression

### Explanatory Variable

The Explanatory Variables used for this model are:

* GenHlth
* Age
* HighChol
* Sex
* Stroke
* HighBP
* Smoker
* Diabetes
* DiffWalk
* NoDocbcCost
* MentHlth
* CholCheck
* PhysHlth
* HvyAlcoholConsump
* Veggies

### Response Variable

The Response Variable in this data set is **_HeartDiseaseorAttack_**

### Logistic Regression Model

```{r}
# Set Seed for Consistency
set.seed(123)

# Split Data According to Train and Test to reduce Overfitting data
trainIndex <- createDataPartition(clean_data$HeartDiseaseorAttack, p = 0.7, list = FALSE)
train <- clean_data[trainIndex, ]
test <- clean_data[-trainIndex, ]
```

```{r}
# Create a formula for the logistic regression
formula <- as.formula("HeartDiseaseorAttack ~HighBP+HighChol+CholCheck+BMI+Smoker+Stroke+Diabetes+PhysActivity+Fruits+Veggies+HvyAlcoholConsump+AnyHealthcare+NoDocbcCost+GenHlth+MentHlth+PhysHlth+DiffWalk+Sex+Age+Education")

# Fit the logistic regression model using forward selection and cross-validation
fitControl <- trainControl(method = "cv", number = 10)
model <- train(formula, data = train, method = "glmStepAIC", trControl = fitControl, tuneLength = 1, direction = "forward", family = "binomial", trace = FALSE)
```

```{r}
summary(model)
```

* All the 21 Variables are given to the model to choose the ones that have the most impact.
* Out of the 21, 15-16 Variables with a significant impact are selected for the model by forward selection method.
* Forward selection varies every time its run, therefore this process was repeated multiple times to find an average AIC.
* The Model Gives an average AIC of 31500, which seem to increase if any of the significant variables are removed. Thus making the model worse.

### Model Equiation

From the Model, the Equation obtained is:

$logodds(HeartDiseaseorAttack) = -5.018184 + (0.474585 * Age3) + (0.544970 * Age4) + (0.825860 * Age5) + \\(1.070209 * Age6) + (1.436164 * Age7) + (1.600890 * Age8) + (1.985744 * Age9) + (2.212924 * Age10) + \\(2.507390 * Age11) + (2.786944 * Age12) + (3.064947 * Age13) + (0.455312 * GenHlth2) + \\(0.973855 * GenHlth3) + (1.521077 * GenHlth4) + (1.813772 * GenHlth5) + (0.534430 * HighBP1) + \\(0.771859 * Sex1) + (1.189973 * Stroke1) + (0.665349 * HighChol1) + (0.372099 * Smoker1) + \\(0.318601 * DiffWalk1) + (0.297784 * Diabetes2) + (0.313842 * NoDocbcCost1) + (0.521134 * CholCheck1) + \\(0.006700 * MentHlth) - (0.125519 * Education6) + (0.004527 * PhysHlth) - \\(0.340506 * HvyAlcoholConsump1) + (0.058462 * Veggies1)$

### What the Intercept means in context of the data

The Intercept for the Logistic Regression model is the log-odds of the individual getting a Heart Disease/Attack when all the predictor variables are 0.

### Purpose of the Intercept

The Intercept is important as it provides the base for comparing the effect of the predictor variables on the response variable. For example consider this following record:

* Age = 4, GenHlth = 1, HighBP = 0, 
* Sex = 1, Stroke = 0, HighChol = 0, 
* Smoker = 0, DiffWalk = 0, Diabetes = 0, 
* NoDocbcCost = 0, CholCheck = 1, MentHlth = 10, 
* Education = 3, Veggies = 0, PhysHlth = 10, HvyAlcoholConsump = 0

If We substitute the above values into the formula we get:

$-5.018184 + (0.544970 * Age4) + (0.771859 * Sex1) + (0.521134 * CholCheck1) + (0.006700 * MentHlth)  + (0.004527 * PhysHlth)$

$-5.018184 + (0.544970 * 1) + (0.771859 * 1) + (0.521134 * 1) + (0.006700 * 10)  + (0.004527 * 10) = -3.067951$
    
The result shows that there is a log-odds of -3.067951 for this individual to get a Heart Disease/Attack, i.e, Less likely to have a Heart Disease or Attack.  

### What the Slope means in context of the data

* In the context of this data set, the Slope of a predictor variable shows the change in log-odds of the response (Heart Disease or Attack) per unit increase in the predictor variable, while holding all other predictor variables as constant.
* For instance, a unit change in the predictor variable **_PhysHlth_**, while all other 15 variables are constant, will increase the log-odds of the individual having a heart disease or attack by 0.004527.


## MODEL DIAGNOSTICS

### Columns for probability predition and residuals.

Creating a prediction column in the test data in terms of probability instead of log(odds)

```{r}
# Predict the model
test$prob_prediction <- predict(model, newdata = test, type = "prob")

# Subset only Probability of HeartDiseaseorAttack = 1
test$prob_prediction <- test$prob_prediction[["1"]]
```

Observing the Response variable according to the predicted probability

```{r}
# Subset Response variable and the calculated probability
result <- test[, c("HeartDiseaseorAttack", "prob_prediction")]

# Isolating both conditions of Heart Disease or Attack
t_res <- result %>% filter(HeartDiseaseorAttack == 1)
f_res <- result %>% filter(HeartDiseaseorAttack == 0)
```

```{r}
# Summary of the Result when HeartDiseaseorAttack is 0
summary(f_res)
```

It is observed that when the prob_prediction is on average 9% to 50% with some exceptions all the way to 99%

```{r}
# Summary of the Result when HeartDiseaseorAttack is 1
summary(t_res)
```

It is observed that when the prob_prediction is on average 55% to 90% with some exceptions all the way from 1%

#### Using the prediciton model on the original dataset

```{r}
# Add Prediction Column
clean_data$prob_prediction <- predict(model, newdata = clean_data, type = "prob")

# Keeping only the HeartDiseaseorAttack == 1
clean_data$prob_prediction <- clean_data$prob_prediction[["1"]]

# Add Residual Column
clean_data$residuals <- as.numeric(clean_data$HeartDiseaseorAttack) - clean_data$prob_prediction

str(clean_data)
```

### Independence of Residuals & Constant Variability

```{r}
plot(fitted(model$finalModel), resid(model$finalModel), ylab = "Residuals", xlab = "Fitted values", main = "Residuals vs. Fitted values")
```

### Correlation of explanatory variables

Since out of the 16 variables selected, only 2 are continuous, the correlation matrix is created for these two variables.

```{r}
cor_matrix <- cor(clean_data[,c(16,17)])
cor_matrix
```

The Correlation between these two variables is 0.37, which represents a weak positive correlation.

```{r message=FALSE, warning=FALSE}
ggplot(clean_data, aes(PhysHlth, MentHlth)) +
  geom_point() +
  geom_smooth(method = "glm", se = FALSE) +
  labs(
    title = "Correlation of Mental Health and Physical Health",
    y = "Mental Health",
    x = "Physical Health",
    caption = 'Soruce:BRFSS 2015 | Kaggle'
  )
```


### Influencial Points in Model

```{r}
# Plot influence plot
influencePlot(model$finalModel, main="Influencial Points in Model")
```

## Accuracy of the model

```{r}
accuracy <- confusionMatrix(as.factor(round(clean_data$prob_prediction)), clean_data$HeartDiseaseorAttack)$overall[1]
# Print the accuracy and AUC
cat("Accuracy:", accuracy, "\n")
```

## CONCLUSION

* This model has an efficient AIC as forward selection is used along with cross validation to prevent oversampling of data.
* Constant Variability, and Independence of Residuals are violated as the plot shows that there is some form of relationship between the fitted and residual models.
* The Response variables however are not strongly correlated to each other, with only 0.37 as the correlation.
* The Influence plot shows that most of the influential points are within in the confined limits of the model.
* The model has an accuracy of around 75-80%, which may be satisfactory, but definitely can be improved.

## References

1.  https://www.cdc.gov/brfss/annual_data/annual_2015.html
2.  https://www.cdc.gov/brfss/annual_data/2015/pdf/codebook15_llcp.pdf
3.  https://www.kaggle.com/code/alexteboul/heart-disease-health-indicators-dataset-notebook
4.  https://r-graph-gallery.com/index.html
5.  http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html
6.  https://stulp.gmw.rug.nl/ggplotworkshop/twodiscretevariables.html
7.  https://datascott.com/blog/subtitles-with-ggplotly/
8.  https://r-graphics.org/recipe-facet-label-text
9.  https://towardsdatascience.com/data-visualization-101-how-to-choose-a-chart-type-9b8830e558d6
10. http://www.sthda.com/english/wiki/ggplot2-axis-ticks-a-guide-to-customize-tick-marks-and-labels
11. https://www.youtube.com/watch?v=qnw1xDnt_Ec
12. https://www.youtube.com/watch?v=rBp3eYHrsfo
13. https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf
14. https://www.youtube.com/watch?v=uFUM9M97Me8
15. https://topepo.github.io/caret/
16. https://rmarkdown.rstudio.com/lesson-7.html
17. https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html
18. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/
19. https://www.youtube.com/watch?v=g0TfqTF127M
20. https://www.youtube.com/watch?v=C4N3_XJJ-jU
21. https://www.analyticsvidhya.com/blog/2015/11/beginners-guide-on-logistic-regression-in-r/
22. https://www.r-bloggers.com/2019/09/heart-disease-prediction-from-patient-data-in-r/
23. https://rpubs.com/Edenner/854339
24. https://online.stat.psu.edu/stat501/lesson/r-help-11-influential-points
25. https://rpubs.com/mpfoley73/501093

